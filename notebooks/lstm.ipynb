{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0ad4b3",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "In this notebook the LSTM model will be implemented. We will use historical energy consumption data and meteorological data to make a prediction on future consumption values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c6fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "seed=99\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5c1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data\n",
    "morocco = pd.read_csv(\"../data/processed/morocco_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf668fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "morocco.set_index('DateTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ec2cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>general diffuse flows</th>\n",
       "      <th>diffuse flows</th>\n",
       "      <th>Consumption</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:00:00</th>\n",
       "      <td>6.559</td>\n",
       "      <td>73.8</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.119</td>\n",
       "      <td>70425.53544</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:10:00</th>\n",
       "      <td>6.414</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.085</td>\n",
       "      <td>69320.84387</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:20:00</th>\n",
       "      <td>6.313</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.100</td>\n",
       "      <td>67803.22193</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:30:00</th>\n",
       "      <td>6.121</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.096</td>\n",
       "      <td>65489.23209</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:40:00</th>\n",
       "      <td>5.921</td>\n",
       "      <td>75.7</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.085</td>\n",
       "      <td>63650.44627</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Temperature  Humidity  Wind Speed  general diffuse flows  \\\n",
       "DateTime                                                                        \n",
       "2017-01-01 00:00:00        6.559      73.8       0.083                  0.051   \n",
       "2017-01-01 00:10:00        6.414      74.5       0.083                  0.070   \n",
       "2017-01-01 00:20:00        6.313      74.5       0.080                  0.062   \n",
       "2017-01-01 00:30:00        6.121      75.0       0.083                  0.091   \n",
       "2017-01-01 00:40:00        5.921      75.7       0.081                  0.048   \n",
       "\n",
       "                     diffuse flows  Consumption  year  month  day  hour  \n",
       "DateTime                                                                 \n",
       "2017-01-01 00:00:00          0.119  70425.53544  2017      1    1     0  \n",
       "2017-01-01 00:10:00          0.085  69320.84387  2017      1    1     0  \n",
       "2017-01-01 00:20:00          0.100  67803.22193  2017      1    1     0  \n",
       "2017-01-01 00:30:00          0.096  65489.23209  2017      1    1     0  \n",
       "2017-01-01 00:40:00          0.085  63650.44627  2017      1    1     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null values: 0\n",
      "\n",
      "data types: Temperature              float64\n",
      "Humidity                 float64\n",
      "Wind Speed               float64\n",
      "general diffuse flows    float64\n",
      "diffuse flows            float64\n",
      "Consumption              float64\n",
      "year                       int64\n",
      "month                      int64\n",
      "day                        int64\n",
      "hour                       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "display(morocco.head())\n",
    "print('null values:', morocco.isnull().sum().sum())\n",
    "print()\n",
    "print('data types:', morocco.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553e41e",
   "metadata": {},
   "source": [
    "### Literature review\n",
    "In the previous thesis written by Andreas:  \n",
    "* Scikit StandardScaler\n",
    "* 48h lookback horizon\n",
    "* Hyperparameter tuning on lookback horizon and number of layers using grid search.\n",
    "* The parameters to adjust was selected so that the training would use less than 12h on a NVIDIA P100 graphics card.\n",
    "* Did not use cross-validation given time series data. Instead used train-test split using the last 90 days for test. This makes sense, and is kinda how I selected my data.\n",
    "* Batch size 32, max 150 epochs, learning rate schedulers on plateau, initial lr=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b599435",
   "metadata": {},
   "source": [
    "## Split data into training and test sets\n",
    "The Morocco dataset contains one year of data of 10 min granularity. In order to respect the temporal order of observations we'll split on a given timestep and use the data before the respective timestamp as training data and the data after will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd97652d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m split_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2017-12-01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m12\u001b[39m)]\n\u001b[1;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m)]\n\u001b[1;32m      6\u001b[0m X_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumption\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "split_date = pd.Timestamp('2017-12-01')\n",
    "\n",
    "train = morocco[(morocco['month'] < 12)]\n",
    "test = morocco[(morocco['month'] >= 12)]\n",
    "\n",
    "X_train = train.drop(columns=['Consumption'])\n",
    "y_train = train['Consumption']\n",
    "\n",
    "X_test = test.drop(columns=['Consumption'])\n",
    "y_test = test['Consumption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5f16e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (48096, 9)\n",
      "y_train: (48096,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a41dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>general diffuse flows</th>\n",
       "      <th>diffuse flows</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-08 17:50:00</th>\n",
       "      <td>25.58</td>\n",
       "      <td>64.65</td>\n",
       "      <td>0.259</td>\n",
       "      <td>254.000</td>\n",
       "      <td>35.740</td>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-02 21:30:00</th>\n",
       "      <td>25.67</td>\n",
       "      <td>59.40</td>\n",
       "      <td>4.903</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.126</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27 18:30:00</th>\n",
       "      <td>22.45</td>\n",
       "      <td>80.20</td>\n",
       "      <td>4.921</td>\n",
       "      <td>28.620</td>\n",
       "      <td>27.360</td>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-13 06:00:00</th>\n",
       "      <td>21.92</td>\n",
       "      <td>76.50</td>\n",
       "      <td>4.919</td>\n",
       "      <td>2.311</td>\n",
       "      <td>1.882</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-28 13:40:00</th>\n",
       "      <td>23.50</td>\n",
       "      <td>41.31</td>\n",
       "      <td>0.087</td>\n",
       "      <td>884.000</td>\n",
       "      <td>55.770</td>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Temperature  Humidity  Wind Speed  general diffuse flows  \\\n",
       "DateTime                                                                        \n",
       "2017-09-08 17:50:00        25.58     64.65       0.259                254.000   \n",
       "2017-08-02 21:30:00        25.67     59.40       4.903                  0.073   \n",
       "2017-09-27 18:30:00        22.45     80.20       4.921                 28.620   \n",
       "2017-07-13 06:00:00        21.92     76.50       4.919                  2.311   \n",
       "2017-04-28 13:40:00        23.50     41.31       0.087                884.000   \n",
       "\n",
       "                     diffuse flows  year  month  day  hour  \n",
       "DateTime                                                    \n",
       "2017-09-08 17:50:00         35.740  2017      9    8    17  \n",
       "2017-08-02 21:30:00          0.126  2017      8    2    21  \n",
       "2017-09-27 18:30:00         27.360  2017      9   27    18  \n",
       "2017-07-13 06:00:00          1.882  2017      7   13     6  \n",
       "2017-04-28 13:40:00         55.770  2017      4   28    13  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460d0d7",
   "metadata": {},
   "source": [
    "### Tensorflow implementation\n",
    "You should have your data split into input sequences (X) and the corresponding labels (y). Here, X is expected to be a 3D array of shape (number of samples, time steps, features per step), and y is a 2D array of shape (number of samples, target variable).\n",
    "\n",
    "Replace 50 with the number of LSTM units you want. The input_shape parameter should match the shape of your input data, excluding the sample dimension (e.g., (time steps, features per step)).\n",
    "\n",
    "Here, X.shape[1] is the number of time steps, and X.shape[2] is the number of features per time step. Adjust the epochs and validation_split as necessary.\n",
    "\n",
    "Assuming X_test is your test dataset prepared in the same way as your training dataset (X).\n",
    "\n",
    "Remember, the effectiveness of your model heavily depends on the quality of your data, the way you've preprocessed it, and the hyperparameters of the model. It's often beneficial to experiment with different configurations, LSTM layers, and maybe adding dropout layers to improve the model's performance and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6f2b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94069cd0",
   "metadata": {},
   "source": [
    "The LSTM model is defined with input_shape=(n_steps, n_features), which expects 3-dimensional input. Cannot directly pass X_train and y_train to the models fit() function without reshaping them to include the time steps dimension. You need to reshape your `X_train` and `X_test` data into the 3-dimensional shape expected by the LSTM layers. This is typically done by segmenting your time series data into sequences.  \n",
    "\n",
    "Tensorflow introduces TimeseriesGenerator, which automatically handle the segmentation of time series data into (samples, time steps, features) format that LSTM layers expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseriesgenerator is deprecated. Here is the manual code.\n",
    "def create_sequences(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "n_steps = lookback  # time steps parameter\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled.flatten(), n_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled.flatten(), n_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b5b8a",
   "metadata": {},
   "source": [
    "Selecting the window size/ n_steps. Pick one that allows the model to learn long-term dependencies, but hyptuning isn't neccessary. We perform short term forecasting, so we will use 2 days = 48 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dafaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    elif epoch%10==0:\n",
    "        return lr * tf.math.exp(-0.3)\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "def create_lstm_model(X_train, X_test, y_train, y_test, n_steps, n_features, n_epochs, n_layers):\n",
    "    model = Sequential()\n",
    "    for _ in range(n_layers - 1):\n",
    "        model.add(LSTM(32, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    \n",
    "    model.add(LSTM(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    # model.compile(optimizer='adam', loss='mse')\n",
    "    model.compile(optimizer=Adam(learning_rate = 0.001), loss='mse')\n",
    "    \n",
    "    callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)]\n",
    "    \n",
    "    #model.fit(train_generator, epochs=n_epochs, verbose=1, validation_data=test_generator, callbacks=callbacks)\n",
    "    model.fit(X_train, y_train, epochs=n_epochs, verbose=1, callbacks=callbacks)\n",
    "    return model\n",
    "\n",
    "def fitting_and_eval(features, target, dataset_name, model_file_identifier='_', lookback = 6, n_layers = 2):\n",
    "    # split_and_scale: Test set is the last 90 days\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=2160, random_state=seed, shuffle=False)\n",
    "    \n",
    "    # Scale X data\n",
    "    XScaler = StandardScaler()\n",
    "    XScaler.fit(X_train)\n",
    "    X_train_scaled = XScaler.transform(X_train)\n",
    "    X_test_scaled = XScaler.transform(X_test)\n",
    "\n",
    "    # Scale Y data\n",
    "    YScaler = StandardScaler()\n",
    "    YScaler.fit(y_train.values.reshape(-1, 1))\n",
    "    y_train_scaled = YScaler.transform(y_train.values.reshape(-1, 1))\n",
    "    y_test_scaled = YScaler.transform(y_test.values.reshape(-1, 1))\n",
    "    \n",
    "    # Variables\n",
    "    n_steps = lookback # hours\n",
    "    batch = 64\n",
    "    epochs = 150\n",
    "    #generator_train = TimeseriesGenerator(scaled_X_train, scaled_y_train, length=n_steps, batch_size=batch)\n",
    "    #generator_test = TimeseriesGenerator(scaled_X_test, scaled_y_test, length=n_steps, batch_size=batch)\n",
    "    \n",
    "    \n",
    "    model_file_path = \"lstm/\" + model_file_identifier + \"/\" + dataset_name\n",
    "    try: # Try to load the model if it already exists\n",
    "        model = tf.keras.models.load_model(model_file_path)\n",
    "    except: # If it doesn't exist, train the model and save it\n",
    "        n_features = X_train.shape[1]\n",
    "        model = create_lstm_model(X_train, X_test, y_train, y_test, n_steps, n_features, epochs, n_layers)\n",
    "        model.save(model_file_path)\n",
    "    \n",
    "    # evaluate the model on the test set\n",
    "    y_pred_scaled = model.predict(generator_test)\n",
    "    y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
    "    mape = mean_absolute_percentage_error(y_test[n_steps:], y_pred)\n",
    "    mae = mean_absolute_error(y_test[n_steps:], y_pred)\n",
    "    r2 = r2_score(y_test[n_steps:], y_pred)\n",
    "    print(f'{dataset_name}, MAE: {mae:.2f}, MAPE: {mape:.3f}, R2: {r2:.3f}')\n",
    "\n",
    "    return r2, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88986a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariuslerstein/miniconda3/envs/master/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 8), dtype=float32). Expected shape (None, 6, 8), but input has incompatible shape (None, 8)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 8), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m, in \u001b[0;36mfitting_and_eval\u001b[0;34m(features, target, dataset_name, model_file_identifier, lookback, n_layers)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m: \u001b[38;5;66;03m# If it doesn't exist, train the model and save it\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 54\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_lstm_model(X_train, X_test, y_train, y_test, n_steps, n_features, epochs, n_layers)\n\u001b[1;32m     55\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(model_file_path)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# evaluate the model on the test set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mcreate_lstm_model\u001b[0;34m(X_train, X_test, y_train, y_test, n_steps, n_features, n_epochs, n_layers)\u001b[0m\n\u001b[1;32m     19\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mLearningRateScheduler(scheduler, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#model.fit(train_generator, epochs=n_epochs, verbose=1, validation_data=test_generator, callbacks=callbacks)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mn_epochs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.12/site-packages/keras/src/models/functional.py:280\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    279\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 8), dtype=float32). Expected shape (None, 6, 8), but input has incompatible shape (None, 8)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 8), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = morocco.dropna().reset_index(drop=True)\n",
    "lookback=48 # 2 days\n",
    "n_layers=2\n",
    "model_file_identifier = str(lookback) + '_' + str(n_layers)\n",
    "\n",
    "mape = fitting_and_eval(data.drop(columns=['Consumption', 'Temperature']), data['Consumption'], 'morocco', model_file_identifier=model_file_identifier, lookback=lookback, n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fff09d",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "Bayesian search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning done on 50 epochs 48 lagged steps\n",
    "hyperparameter_tuning = {}\n",
    "for lookback in [6, 12, 24]:\n",
    "    for n_layers in [1, 2]:\n",
    "        print(f'Hyperparameters: lookback={lookback}, n_layers={n_layers}')\n",
    "        parameter_results = []\n",
    "        for dataset_name in datasets_electricity:\n",
    "            data = add_lagged_timesteps(datasets[dataset_name], lag_periods=[i for i in range(1, 49)], lagged_feature='Consumption').dropna().reset_index(drop=True)\n",
    "            mape = lstm_fitting_and_evaluation(data.drop(columns=['DateTime', 'Consumption', 'Temperature']), data['Consumption'], dataset_name, model_file_identifier=str(lookback) + \"_\" + str(n_layers), lookback=lookback, n_layers=n_layers)\n",
    "            parameter_results.append(mape)\n",
    "        hyperparameter_tuning[\"Lookback:\"+str(lookback) + ' ' + \"Layers:\"+str(n_layers)] = parameter_results\n",
    "\n",
    "df = pd.DataFrame(hyperparameter_tuning,\n",
    "                    index=[i for i in datasets_electricity.keys()])\n",
    "df.plot.bar(figsize=(10, 5))\n",
    "plt.title('MAPE for different hyperparameters with LSTM')\n",
    "plt.ylabel('MAPE')\n",
    "plt.xlabel('Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb591961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "df = pd.DataFrame(hyperparameter_tuning,\n",
    "                    index=[i for i in datasets_electricity.keys()])\n",
    "df.plot.bar(figsize=(10, 5))\n",
    "plt.title('MAPE for different hyperparameters with LSTM')\n",
    "plt.ylim(0, 0.1)\n",
    "plt.ylabel('MAPE')\n",
    "plt.xlabel('Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae07de5",
   "metadata": {},
   "source": [
    "### PyTorch implementation\n",
    "f you prefer not to use TensorFlow for building an LSTM model, another popular choice is PyTorch, a flexible deep learning framework that allows more explicit control over the model architecture and data flow. Here is a basic example of how to implement an LSTM for time series forecasting in PyTorch.\n",
    "\n",
    "In PyTorch, you define your model as a class that extends nn.Module. Below is a simple example.\n",
    "\n",
    "Your data should be formatted appropriately for PyTorch, typically as torch.Tensor objects. The input should be of shape (batch size, sequence length, number of features), and the labels should be of a compatible shape, depending on your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97c99fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3827d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acce208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373919b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[:, -1, :] just means we are taking the last LSTM output for each sequence\n",
    "out = self.linear(out[:, -1, :]) \n",
    "return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "input_dim = 1  # number of features\n",
    "hidden_dim = 50\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "\n",
    "model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim)\n",
    "\n",
    "# Mean Squared Error Loss\n",
    "criterion = torch.nn.MSELoss()   \n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2cd8a2",
   "metadata": {},
   "source": [
    "Training the model involves running the forward pass, calculating the loss, performing backpropagation, and updating the model parameters.\n",
    "\n",
    "In this code, X_train and y_train should be your training data and labels, respectively, formatted as PyTorch tensors. Ensure that the shapes of your data match the expectations of the model (X_train should have three dimensions: batch, sequence, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96969026",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients w.r.t. parameters\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()  # Getting gradients w.r.t. parameters\n",
    "    optimizer.step()  # Updating parameters\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173875bb",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "After training, you can use the model to predict on new data. Ensure you format this data similarly to the training data before making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc3c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions = model(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6723a",
   "metadata": {},
   "source": [
    "Here, X_test should be your test dataset, prepared in the same way as your training dataset.\n",
    "\n",
    "This example provides a basic introduction to implementing an LSTM in PyTorch for time series forecasting. Depending on your specific problem, you might need to adjust the model architecture, data preprocessing, or training process for optimal results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
